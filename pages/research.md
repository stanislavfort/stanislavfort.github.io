---
title: Research
---

I am interested in emergence, AI, and physics. My current focus is on 1) (empirical) theories of deep learning & deep learning understanding, and 2) applying deep learning methods to the physical sciences, especially astrophysics and quantum. I'm especially keen on neural network scaling & its benefits.

<tr>
<td width="25%">
<img src="/images/subspaces.png" alt="arxiv.org/abs/2107.05802" style="width:30%;float:left;margin:0em 0em 0em 3em">
</td>

<td width="75%" valign="top">
### [19. How many degrees of freedom do we need to train deep networks: a loss landscape perspective](https://arxiv.org/abs/2107.05802)
Brett W. Larsen, **Stanislav Fort**, Nic Becker, Surya Ganguli


Deep neural networks are capable of training and generalizing well in many low-dimensional manifolds in their weights.  We explain this phenomenon by first examining the success probability of hitting a training loss sublevel set when training within a random subspace of a given training dimensionality using Gordon's escape theorem.
</td>
</tr>

<tr>
<td width="25%">
<img src="/images/maha_ratio.jpg" alt="arxiv.org/abs/2107.05802" style="width:30%;float:left;margin:0em 0em 0em 3em">
</td>

<td width="75%" valign="top">
### [18. A Simple Fix to Mahalanobis Distance for Improving Near-OOD Detection](https://arxiv.org/abs/2106.09022)
Jie Ren, **Stanislav Fort**, Jeremiah Liu, Abhijit Guha Roy, Shreyas Padhy, Balaji Lakshminarayanan

We analyze the failure modes of the Mahalanobis distance method for near-OOD detection and propose a simple fix called relative Mahalanobis distance (RMD) which improves performance and is more robust to hyperparameter choice.
</td>
</tr>
